---
layout: post
title:  "Currently Reviewing Papers for NIPS 2016"
date:   2016-06-25 10:00:00
permalink: 2016-06-25-currently-reviewing-papers-for-nips-2016/
---

This year, I volunteered to be a NIPS reviewer, since I want to get the experience of reviewing
academic papers. It was pretty easy to volunteer: for every paper submitted to NIPS, they asked that
at least one of the authors offer to be a reviewer, and preferably *not* someone who's already on
their list of known reviewers. I'm clearly not among that elite crowd, and I wanted to review
anyway, so it was an easy choice for me.

This year, NIPS had around 2500 (!) paper submissions, according to the number I saw in the
Conference Management Toolkit website, which is what NIPS uses to manage paper submissions and
reviews. I think the exact number I saw was roughly 2450-ish, but it's worth noting that papers with
authors who match my conflict of interest domains (especially "berkeley.edu") wouldn't appear on
this list. When I was submitting my paper bids for reviewing purposes, I couldn't find the paper I
submitted (fortunately!). 

For obvious reasons, I can't divulge too much about the papers I'm reviewing. But I can probably
safely say this: I got assigned four papers to review in the span of roughly a month (from June 17
to July 17). So I'm reviewing right now, and decided to write this blog post during one of my
breaks. Now that I've read all my assigned papers and have more-or-less come up with a very
high-level draft conclusion about them, here are four thoughts:

1. Unfortunately, I'm probably deeply qualified to review ... none of the four papers. For two of
the papers, I understand much of the main ideas, but I still lack intricate knowledge of the math
and algorithms they use, and about the most prevalent related work. For two other papers, I'm close
to being lost on them. Therefore, I have made this a rule: for each paper, I will allow myself to
read one extra reference for background information. One. So effectively, my reading load for this
cycle is eight papers, four to be read in-depth and four to be read semi-in-depth.  This raises my
curiosity: *how do people actually carefully review papers*? The vast majority of NIPS papers are
extraordinarily specialized and technical, and many still have proofs in appendices. I don't see how
it is humanly possible for me to verify the proofs and make sure the math lines up correctly,
especially with an internship that already takes up about 50 hours a week. It can take hours just
to verify one simple line of equations in a paper, which might require looking through a previous
paper reference, checking Wikipedia for additional background information, and then marking down an
annoying typo that got me confused. Can an experienced NIPS reviewer please tell me how he or she
reviews papers?

2. Fortunately, the previous thought might not matter because, despite not having gone through all
the technical material so far (I've only been reviewing a week!) I feel like I can more or less tell
how I feel about the papers. For instance, the amount of formalism and "cleanliness" of the paper
gives a good clue about its quality. If the paper is rife with typos and sloppy LaTeX, or if the
figures look spectacularly low-quality, that's just asking for a rejection. (Come on, [with
matplotlib embedded in Jupyter notebooks][2], people should be able to produce acceptable figures.)
Note that by this, I'm *excluding* language issues that might arise due to non-native English
speakers writing the paper.  It's not fair to them.

3. It's nice reviewing papers without knowing the identity of the authors. NIPS is double blind,
which I believe to be a good thing, and I say this as someone who would probably *benefit* from an
open reviewing process. I don't want to think about the authors' identities. I have not actively
searched so I'm still in the dark about this for all four of my papers. As a reviewer, I like my
identity being protected, so I'm reconsidering my earlier thought about [making some of the reviews
public][1] (sadly, if this were politics, I'd be accused of "waffling"). Unfortunately, for one of
my papers, I have a good feeling about who at least one of the authors is based on previous
research, despite how I haven't been in this field very long (and some would argue, have *never*
been in this field). In addition, *two* of my other papers have a previous reference which is
obviously similar to the current work, and since I ended up using those papers as my "background
reference" material and seeing practically word-for-word similarities ... author similarities
follow. I'll keep my guesses in mind and then see how they turn out later in August.

4. Despite my internship, I would bet I have more time on my hands than most faculty for reviewing,
because I have fewer competing priorities and because I would be assigned fewer papers (hopefully
... *did I*?).  Thus, I will try to write some *really detailed suggestions* or at the very least,
thought-provoking comments/questions, and I think the authors would appreciate that even if their
papers don't get accepted. At all costs, I am *not* going to be writing those dreaded three line
reviews that contribute nothing. Sadly, I frequently see these online at the NIPS proceedings, which
makes reviews public for accepted papers. As a result, this period from mid-June to mid-July is
going to be super busy for me, since I am doing all my reviewing in nights and weekends.  I am
really trying to make sure my reviews are helpful.

That's all for now --- I need to get back to reviewing.

[1]:http://danieltakeshi.github.io/2015/02/28/make-the-best-peer-reviews-public/
[2]:http://danieltakeshi.github.io/2016-01-16-ipython-jupyter-notebooks-and-matplotlib/
