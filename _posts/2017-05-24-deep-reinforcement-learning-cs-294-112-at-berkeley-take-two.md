---
layout:     post
title:      "Deep Reinforcement Learning (CS 294-112) at Berkeley, Take Two"
date:       2017-05-24 20:00:00
permalink:  2017/05/24/deep-reinforcement-learning-cs-294-112-at-berkeley-take-two
---

Back in Fall 2015, I took the first edition of *Deep Reinforcement Learning* (CS
294-112) at Berkeley. As usual, I [wrote a blog post][2] about the class; you
can find more about other classes I've taken by [searching the archives][5].

In that blog post, I admitted that CS 294-112 had several weaknesses, and also
that I didn't quite fully understand the material.  Fast forward to today, and
I'm pleased to say that:

- There has been a second edition of CS 294-112, taught this past spring
  semester. It was a three-credit, full semester course and therefore more
  substantive than the previous edition which was two-credits and lasted only
  eight weeks.  Furthermore, the slides, homework assignments, *and* the lecture
  recordings are all publicly available online. Check out [the course
  website][1] for details. You can find the homework assignments [in this GitHub
  repository][4] (I had to search a bit for this).

- I now understand much more about deep reinforcement learning and about how to
  use TensorFlow.

These developments go hand in hand, because I spent much of the second half of
the Spring 2017 semester self-studying the second edition of CS 294-112. (To be
clear, I was not enrolled in the class.) I know I said I would first self-study
a few other courses [in a previous blog post][3], but I couldn't pass up such a
prime opportunity to learn about deep reinforcement learning. Furthermore, the
field moves so fast that I worried that if I didn't follow what was happening
*now*, I would *never* be able to catch up to the research frontier if I tried
to do so in a year.

The class had four homework assignments, and I completed all of them with the
exception of skipping the DAgger algorithm implementation in the first homework.
The assignments were extremely helpful for me to understand how to better use
TensorFlow, and I finally feel comfortable using it for my personal projects.
If I can spare the time (famous last words) I plan to write some
TensorFlow-related blog posts.

The video lecture were a nice bonus. I only watched a fraction of them, though.
This was in part due to time constraints, but also in part due to the lack of
captions. The lecture recordings are on YouTube, and in YouTube, I can turn on
automatic captions which helps me to follow the material. However, some of the
videos didn't enable that option, so I had to skip those and just read the
slides since I wasn't following what was being said. As far as I remember,
automatic captions are provided as an option so long as whoever uploaded the
video enables some setting, so maybe someone forgot to do so? Fortunately, the
lecture video on policy gradients has captions enabled, so I was able to watch
that one. Oh, and [I wrote a blog post about the material][6].

Another possible downside to the course, though this one is extremely minor, is
that the last few class sessions were *not* recorded, since those were when
students presented their final projects. Maybe the students wanted some level of
privacy? Oh well, I suppose there's way too many other interesting projects
available anyway (by searching GitHubs, arXiv preprints, etc.) to worry about
this thing.

I want to conclude with a huge thank you to the course staff. Thank you for
helping to spread knowledge about deep reinforcement learning with a great class
and with lots of publicly available material. I really appreciate it.

[1]:http://rll.berkeley.edu/deeprlcourse/
[2]:https://danieltakeshi.github.io/2015-12-17-review-of-deep-reinforcement-learning-cs-294-112-at-berkeley/
[3]:https://danieltakeshi.github.io/2016-02-20-the-four-classes-that-i-have-self-studied/
[4]:https://github.com/berkeleydeeprlcourse/homework
[5]:https://danieltakeshi.github.io/archive.html
[6]:https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/
