---
title:      "New? Start Here"
layout:     page
permalink:  /new-start-here.html
---

The easiest way to find something that you'll be interested in is to [look at
the archives][10] and browse the titles, which (I hope) are descriptive. Using
the built-in Google site search there would also be useful.

If you're interested in knowing more about **graduate-level classes at
Berkeley**, I write reviews on all the ones I have taken. Here they are:

- [CS 267, Applications of Parallel Computing][40]
- [CS 280, Computer Vision][39]
- [CS 281A, Statistical Learning Theory][38]
- [CS 287, Advanced Robotics][1]
- [CS 288, Natural Language Processing][37]
- [CS 294-112, Deep Reinforcement Learning][2]
- [CS 294-112, Deep Reinforcement Learning (self-study)][50]
- [CS 294-115, Algorithmic Human-Robot Interaction][29]
- [CS 294-129, Deep Neural Networks][43] (GSI/TA)
- [CS 294-131, Special Topics in Deep Learning][53]
- [EE 227BT, Convex Optimization][3]
- [EE 227C, Convex Optimization and Approximation][4]
- [STAT 210A, Theoretical Statistics (Classical)][28]
- [STAT 210B, Theoretical Statistics (Modern)][52]

When I was preparing for **the AI prelims at Berkeley** (required for PhD
students), I wrote a lot about AI topics. I also wrote a "transcript" of my
prelims.

- [My Prelims][9] *[Transcript]*
- [Miscellaneous Prelim Review (Part 1)][45]
- [Miscellaneous Prelim Review (Part 2)][46]
- [Markov Decision Processes and Reinforcement Learning][5]
- [Perceptrons, SVMs, and Kernel Methods][6]
- [Notes on Exact Inference in Graphical Models][7]
- [Closing Thoughts on Graphical Models][48]
- [The Least Mean Squares Algorithm][8]
- [Expectation-Maximization][35]
- [Hidden Markov Models and Particle Filtering][36]
- [Reading Russell and Norvig][44]
- [Stanford's Linear Algebra Review][47]

I also write a lot about other technical areas, and am attempting to write up
more about my thoughts on various technical research papers. 

**Generic Technical Guides**, not including those related to my prelims study:

- [Basics of Bayesian Neural Networks][55]
- [Mathematical Tricks Commonly Used in Machine Learning and Statistics][49]
- [Going Deeper Into Reinforcement Learning: Fundamentals of Policy Gradients][33]
- [Going Deeper Into Reinforcement Learning: Deep-Q-Networks][30]
- [Going Deeper Into Reinforcement Learning: Q-Learning and Linear Function Approximation][34]
- [Higher Order Local Gradient Computation for Backpropagation in Deep Neural Networks][32]
- [Understanding Generative Adversarial Networks][41]
- [Independent Component Analysis --- A Gentle Introduction][17]
- [Ten Things Python Programmers Should Know][14]

Notes on **Specific Research Papers** (for others, see [this GitHub repository][60]):

- [Read-Through of Multi-Level Discovery of Deep Options][56]
- [Learning to Act by Predicting the Future][57]
- [Minibatch Metropolis-Hastings][58] (a paper that I wrote)
- [Understanding Deep Learning Requires Re-Thinking Generalization][59]
- [Notes on the Generalized Advantage Estimation Paper][42]
- [Some Recent Results on Minibatch Markov Chain Monte Carlo Methods][25]

If you are interested in knowing about **what it's like being deaf** then, while
I obviously can't claim to speak for everyone who has hearing impairments, here
are a few that might be informative:

- [The Obligatory "Can I Lip Read?" Question][19]
- [The BVLC (BAIR) Retreat: Disaster Averted!][18]
- [Advocate for Yourself][13]
- [After a Few Weeks of CART, Why do I Feel Dissatisfied?][16]
- [The Problem with Seminars][15]
- [My Pre-College Education as a Deaf Mainstreamed Student][22]
- [New Closed-Captioning Glasses][23]
- [Hearing Aids: How They Help and How They Fall Short in Group Situations][11]
- [Technical Term Dilemma][21]
- [Why Computer Science is a Good Major for Deaf Students][12]

Finally, I write sometimes about **the books I read**, such as in the following:

- [All the Books I Read in 2017, Plus My Thoughts][61]
- [All the Books I Read in 2016, Plus My Thoughts][26]
- [Thoughts on How to Win Friends and Influence People][54]
- [Alan Turing: The Enigma][51]
- [The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World][27]
- [My Three Favorite Books I Read in 2015][20]

[1]:https://danieltakeshi.github.io/2015-12-21-review-of-advanced-robotics-cs-287-at-berkeley/
[2]:https://danieltakeshi.github.io/2015-12-17-review-of-deep-reinforcement-learning-cs-294-112-at-berkeley/
[3]:https://danieltakeshi.github.io/2015-12-22-review-of-convex-optimization-ee-227bt-at-berkeley/
[4]:https://danieltakeshi.github.io/2016-05-21-review-of-convex-optimization-and-approximation-ee-227c-at-berkeley/
[5]:https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/
[6]:https://danieltakeshi.github.io/2015-08-08-perceptrons-svms-and-kernel-methods.md/
[7]:https://danieltakeshi.github.io/2015-07-12-notes-on-exact-inference-in-graphical-models/
[8]:https://danieltakeshi.github.io/2015-07-29-the-least-mean-squares-algorithm/
[9]:https://danieltakeshi.github.io/2015-09-01-my-prelims/
[10]:https://danieltakeshi.github.io/archive.html
[11]:https://danieltakeshi.github.io/2012/08/06/hearing-aids-how-they-help-and-how-they-fall-short-in-group-situations/
[12]:https://danieltakeshi.github.io/2013/02/06/why-computer-science-is-a-good-major-for-deaf-students/
[13]:https://danieltakeshi.github.io/2015-06-20-advocate-for-yourself/
[14]:https://danieltakeshi.github.io/2013/07/05/ten-things-python-programmers-should-know/
[15]:https://danieltakeshi.github.io/2013/11/10/the-problem-with-seminars/
[16]:https://danieltakeshi.github.io/2014/10/05/after-a-few-weeks-of-cart-why-do-i-feel-dissatisfied/
[17]:https://danieltakeshi.github.io/2015/01/03/independent-component-analysis-a-gentle-introduction/
[18]:https://danieltakeshi.github.io/2016-04-16-the-bvlc-bair-retreat-disaster-averted/
[19]:https://danieltakeshi.github.io/2016-05-24-the-obligatory-can-i-lip-read-question/
[20]:https://danieltakeshi.github.io/2015-12-28-my-three-favorite-books-i-read-in-2015/
[21]:https://danieltakeshi.github.io/2012/02/04/technical-term-dilemma/
[22]:https://danieltakeshi.github.io/2013/08/08/my-pre-college-education-as-a-deaf-mainstreamed-student/
[23]:https://danieltakeshi.github.io/2013/06/28/new-closed-captioning-glasses/
[24]:https://danieltakeshi.github.io/2013/07/12/its-time-to-ditch-powerpoint-and-word-in-favor-of-latex/
[25]:https://danieltakeshi.github.io/2016-06-19-some-recent-results-on-minibatch-markov-chain-monte-carlo-methods/
[26]:https://danieltakeshi.github.io/2016/12/31/all-the-books-i-read-in-2016-plus-my-thoughts-long
[27]:https://danieltakeshi.github.io/2016-05-31-the-master-algorithm-how-the-quest-for-the-ultimate-learning-machine-will-remake-our-world/
[28]:https://danieltakeshi.github.io/2016/12/20/review-of-theoretical-statistics-stat-210a-at-berkeley/
[29]:https://danieltakeshi.github.io/2016/12/20/review-of-algorithmic-human-robot-interaction-cs-294-115-at-berkeley/
[30]:https://danieltakeshi.github.io/2016/12/01/going-deeper-into-reinforcement-learning-understanding-dqn/
[31]:https://danieltakeshi.github.io/2016/12/30/five-years-of-blogging/
[32]:https://danieltakeshi.github.io/2017/01/21/understanding-higher-order-local-gradient-computation-for-backpropagation-in-deep-neural-networks/
[33]:https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/
[34]:https://danieltakeshi.github.io/2016/10/31/going-deeper-into-reinforcement-learning-understanding-q-learning-and-linear-function-approximation/
[35]:https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/
[36]:https://danieltakeshi.github.io/2015-07-18-expectation-maximization/
[37]:https://danieltakeshi.github.io/2015/02/14/review-of-natural-language-processing-cs-288-at-berkeley/
[38]:https://danieltakeshi.github.io/2014/12/30/review-of-statistical-learning-theory-cs-281a-at-berkeley/
[39]:https://danieltakeshi.github.io/2015-05-31-review-computer-vision-berkeley/
[40]:https://danieltakeshi.github.io/2016-05-27-review-of-applications-of-parallel-computing-cs-267-at-berkeley/
[41]:https://danieltakeshi.github.io/2017/03/05/understanding-generative-adversarial-networks/
[42]:https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/
[43]:https://danieltakeshi.github.io/2016/12/19/reflections-on-being-a-gsi-for-deep-neural-networks-cs-294-129-at-berkeley/
[44]:https://danieltakeshi.github.io/2015-06-22-reading-russell-and-norvig/
[45]:https://danieltakeshi.github.io/2015-08-18-miscellaneous-prelim-review-part-1/
[46]:https://danieltakeshi.github.io/2015-08-22-miscellaneous-prelim-review-part-2/
[47]:https://danieltakeshi.github.io/2015-07-26-stanfords-linear-algebra-review/
[48]:https://danieltakeshi.github.io/2015-07-15-closing-thoughts-on-graphical-models/
[49]:https://danieltakeshi.github.io/2017/05/06/mathematical-tricks-commonly-used-in-machine-learning-and-statistics
[50]:https://danieltakeshi.github.io/2017/05/24/deep-reinforcement-learning-cs-294-112-at-berkeley-take-two
[51]:https://danieltakeshi.github.io/2017/05/21/alan-turing-the-enigma
[52]:https://danieltakeshi.github.io/2017/06/26/review-of-theoretical-statistics-stat-210b-at-berkeley/
[53]:https://danieltakeshi.github.io/2017/12/17/review-of-deep-learning-cs-294-131-at-berkeley/
[54]:https://danieltakeshi.github.io/2017/09/17/thoughts-on-how-to-win-friends-and-influence-people/
[55]:https://danieltakeshi.github.io/2017/11/26/basics-of-bayesian-neural-networks/
[56]:https://danieltakeshi.github.io/2017/11/24/ddo/
[57]:https://danieltakeshi.github.io/2017/10/10/learning-to-act-by-predicting-the-future/
[58]:https://danieltakeshi.github.io/2017/08/02/my-bair-blog-post-on-minibatch-metropolis-hastings/
[59]:https://danieltakeshi.github.io/2017/05/19/understanding-deep-learning-requires-rethinking-generalization-my-thoughts-and-notes
[60]:https://github.com/DanielTakeshi/Paper_Notes
[61]:https://danieltakeshi.github.io/2017/12/27/all-the-books-i-read-in-2017-plus-my-thoughts-long
